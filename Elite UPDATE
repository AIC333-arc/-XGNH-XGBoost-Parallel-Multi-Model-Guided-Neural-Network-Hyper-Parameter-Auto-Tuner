# Idea developed by myself, coded by ChatGPT
import threading
import math
import numpy as np
from concurrent.futures import ThreadPoolExecutor
from sklearn.linear_model import Ridge
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from xgboost import DMatrix, Booster

class XGB3DEnsembleSafeConverge:
    def __init__(self, n_models=5, n_trees=50, max_depth=5,
                 learning_rate=0.2, batch_size=2, residual_learning_rate=0.2,
                 mode="hybrid", meta_model=None, n_folds=5):
        assert mode in ["meta", "consensus", "hybrid"], "Invalid mode"
        self.n_models = n_models
        self.n_trees = n_trees
        self.max_depth = max_depth
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.residual_learning_rate = residual_learning_rate
        self.mode = mode
        self.models = [[None]*n_trees for _ in range(n_models)]
        self.base_score = 0.5
        self.lock = threading.Lock()
        self.meta_model = meta_model or Ridge(alpha=1.0)
        self.n_folds = n_folds

    def _get_tree_lr(self, t):
        return self.learning_rate

    def fit(self, X, y):
        self.X = np.array(X, dtype=np.float32)
        self.y = np.array(y, dtype=np.float32)
        n_samples = self.X.shape[0]

        # Initialize base predictions and residuals
        self.model_preds = np.full((self.n_models, n_samples), self.base_score, dtype=np.float32)
        residuals = np.zeros((self.n_models, n_samples), dtype=np.float32)
        self.rmse_progression = []

        def train_tree(m, t, res, lr):
            dmat = DMatrix(self.X, label=res)
            params = {
                "max_depth": self.max_depth,
                "eta": lr,
                "objective": "reg:squarederror",
                "verbosity": 0,
                "tree_method": "hist",
                "device": "cpu",
                "base_score": 0
            }
            bst = Booster(params, [dmat])
            bst.update(dmat, iteration=0)
            pred = bst.predict(DMatrix(self.X))
            return m, t, bst, pred

        # Main training loop
        for t in range(self.n_trees):
            tree_lr = self._get_tree_lr(t)
            ensemble_preds = np.mean(self.model_preds, axis=0)
            shared_residuals = self.y - ensemble_preds
            consensus_weight = t / self.n_trees

            for batch_start in range(0, self.n_models, self.batch_size):
                batch_end = min(batch_start + self.batch_size, self.n_models)
                results = []
                with ThreadPoolExecutor(max_workers=self.batch_size) as executor:
                    futures = []
                    for m in range(batch_start, batch_end):
                        combined_residual = residuals[m].copy()
                        if self.mode in ["meta", "hybrid"]:
                            combined_residual += self.residual_learning_rate * shared_residuals
                        if self.mode in ["consensus", "hybrid"]:
                            combined_residual += self.residual_learning_rate * consensus_weight * (ensemble_preds - self.model_preds[m])
                        futures.append(executor.submit(train_tree, m, t, combined_residual, tree_lr))
                    for f in futures:
                        results.append(f.result())

                for m, t_idx, bst, pred in results:
                    with self.lock:
                        self.models[m][t_idx] = bst
                        self.model_preds[m] += tree_lr * pred
                        update = 0
                        if self.mode in ["meta", "hybrid"]:
                            update += self.residual_learning_rate * shared_residuals
                        if self.mode in ["consensus", "hybrid"]:
                            update += self.residual_learning_rate * consensus_weight * (ensemble_preds - self.model_preds[m])
                        residuals[m] = residuals[m] + update - tree_lr * pred

            self.rmse_progression.append(math.sqrt(mean_squared_error(self.y, np.mean(self.model_preds, axis=0))))

        # --- Safe meta-model training using out-of-fold stacking ---
        X_meta = np.zeros((n_samples, self.n_models), dtype=np.float32)
        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)
        for train_idx, val_idx in kf.split(self.X):
            X_meta[val_idx] = np.column_stack([self.model_preds[m][val_idx] for m in range(self.n_models)])

        self.meta_model.fit(X_meta, self.y)
        self.final_preds = self.meta_model.predict(X_meta)
        return self

    def predict(self, X):
        X = np.array(X, dtype=np.float32)
        base_preds = np.full((self.n_models, X.shape[0]), self.base_score, dtype=np.float32)
        for m in range(self.n_models):
            for bst in self.models[m]:
                if bst is not None:
                    base_preds[m] += self.learning_rate * bst.predict(DMatrix(X))
        meta_features = np.column_stack([base_preds[m] for m in range(self.n_models)])
        return self.meta_model.predict(meta_features)
        
